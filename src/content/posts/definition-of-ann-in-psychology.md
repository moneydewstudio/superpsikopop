---
Title: "Definition of Ann in Psychology"
Description: "Get to know the definition of ann according to psychologists."
Date: 2022-12-30T06:00:00+00:142
Image: "/images/posts/definition-of-ann-in-psychology.jpg"
Authors: ["Robi Maulana"]
Categories: ["Glossary"]
Tags: 
Draft: FALSE
---




> See artificial neural network.

## What is the definition of ANN according to Psychologists?

ANN is See artificial neural network.

An artificial neural network is like a computer program that tries to think and learn like our brains. It is made up of many small parts called neurons that work together to solve problems. Just like how our brains have different sections for different tasks like seeing and hearing, a neural network also has different layers that help it process and understand information.

Imagine you have a bunch of colored marbles and you want to sort them. In an artificial neural network, each marble represents a piece of information. The network takes these marbles and passes them through different layers or filters, just like how you would separate marbles by color using different trays or buckets.

As the marbles pass through each layer, the network learns which colors and patterns are similar or different. Gradually, it becomes better at knowing which bucket to put each marble in. This is similar to how our brains learn from experience and become better at recognizing things over time.

Once the artificial neural network has gone through all the layers, it gives an answer or prediction based on the information it learned from the marbles. For example, it might tell you which bucket has the most red marbles or which bucket contains the most special marbles.

Artificial neural networks are used in many things like recognizing pictures, understanding speech, or even playing games. They help computers make decisions and learn from data, just like we do with our brains.

 

## What are the example case of ANN?

1\. "Deep Learning" by Yoshua Bengio, et al. (2015): This seminal paper introduced the concept of deep learning in artificial neural networks (ANNs), paving the way for multiple breakthroughs in various AI fields such as computer vision and natural language processing.

2\. "A Few Useful Things to Know About Machine Learning" by Pedro Domingos (2012): This paper highlights the importance of feature engineering and the impact it has on the performance of ANNs. It discusses key aspects such as overfitting, bias-variance tradeoff, and the need for labeled training data.

3\. "Dropout: A Simple Way to Prevent Neural Networks from Overfitting" by Nitish Srivastava, et al. (2014): This research introduced the concept of dropout, a regularization technique that addresses overfitting by randomly disabling a proportion of neurons during training. Dropout has become a standard practice in ANNs, improving generalization and performance.

4\. "Generative Adversarial Networks" by Ian Goodfellow, et al. (2014): This paper introduced the concept of generative adversarial networks (GANs), in which two ANNs, a generator and discriminator, compete against each other to generate realistic synthetic data. GANs have revolutionized the field of generative modeling and have been used in various applications, including image synthesis and unsupervised learning.

5\. "Visualizing and Understanding Convolutional Networks" by Matthew D. Zeiler and Rob Fergus (2014): This research proposed a technique called "Deconvolutional Network" to visualize the learned feature representations in convolutional neural networks (CNNs). It provided insights into how CNNs perceive visual patterns, leading to improved interpretability and understanding of ANN models.

6\. "Attention is All You Need" by Vaswani, et al. (2017): This paper introduced a novel architecture called the Transformer, which employs self-attention mechanisms to capture dependencies between different words in a sequence. Transformers have revolutionized natural language processing tasks and have outperformed traditional recurrent neural networks (RNNs) on machine translation, text summarization, and language understanding tasks.

7\. "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" by Devlin, et al. (2018): This research introduced the Bidirectional Encoder Representations from Transformers (BERT) model, which achieved state-of-the-art results on various natural language processing benchmarks. BERT demonstrated the effectiveness of language model pre-training and transfer learning, significantly advancing the field.

8\. "Generative Pre-trained Transformer 2" by Radford, et al. (2020): This research presented GPT-2, a large-scale transformer-based language model capable of generating coherent and contextually relevant text. GPT-2 pushed the boundaries of language generation and has raised ethical concerns regarding the potential misuse of extremely powerful AI models.

 

## What are other terms related to ANN?

1\. Feedforward Neural Network: A neural network model where information flows only in the forward direction, from input to output, without any loops or recurrent connections. 2. Convolutional Neural Network (CNN): A type of neural network model that is particularly effective in image and video recognition tasks, using convolutional layers and pooling operations. 3. Recurrent Neural Network (RNN): A neural network model that has feedback connections, allowing information to flow in loops. RNNs are often used for sequential data tasks such as speech recognition and natural language processing. 4. Long Short-Term Memory (LSTM): A type of RNN architecture that overcomes the vanishing gradient problem commonly encountered in traditional RNNs, making it more suitable for processing long sequences of data. 5. Deep Learning: A subset of machine learning that focuses on utilizing neural networks with multiple layers (deep networks) to learn and extract features from large and complex datasets. 6. Backpropagation: A gradient-based optimization algorithm used to train neural networks. It involves calculating the gradient of the network's parameters with respect to a loss function, and then using this information to update the parameters in order to minimize the loss. 7. Activation Function: A mathematical function applied at each neuron in a neural network, which determines the output of that neuron based on its input. Common activation functions include sigmoid, tanh, and rectified linear unit (ReLU). 8. Loss Function: A function that measures the discrepancy between the predicted output of a neural network and the true output, serving as an objective to be minimized during training. Common loss functions include mean squared error (MSE) and cross-entropy loss. 9. Dropout: A regularization technique commonly used in neural networks, where randomly selected neurons are temporarily ignored during training, helping to prevent overfitting and increase generalization. 10. Transfer Learning: A technique that allows a pre-trained neural network to be used as a starting point for a new task, by leveraging the knowledge and learned features of the pre-trained model.

 

## References for ANN

1\. Rumelhart, D.E., Hinton, G.E., & Williams, R.J. (1986). Learning representations by back-propagating errors. Nature, 323(6088), 533-536.

2\. LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444.

3\. Haykin, S. (1994). Neural networks: a comprehensive foundation. Prentice Hall.

4\. Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks, 61, 85-117.

5\. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT Press.

6\. Bishop, C.M. (1995). Neural networks for pattern recognition. Oxford university press.

7\. Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems, 2(4), 303-314.

8\. Hebb, D.O. (1949). The organization of behavior: A neuropsychological theory. Psychology Press.

9\. Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8), 1735-1780.

10\. Graves, A., Mohamed, A.R., & Hinton, G. (2013). Speech recognition with deep recurrent neural networks. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 6645-6649). IEEE.

---
Title: "Definition of Back Propagation in Psychology"
Description: "Get to know the definition of back propagation according to psychologists."
Date: 2023-04-13T06:00:00+00:275
Image: "/images/posts/definition-of-back-propagation-in-psychology.jpg"
Authors: ["Robi Maulana"]
Categories: ["Glossary"]
Tags: 
Draft: FALSE
---




> A learning mechanism used in connectionist modelling in which actual responses are compared to correct ones.

## What is the definition of back-propagation according to Psychologists?

back-propagation is A learning mechanism used in connectionist modelling in which actual responses are compared to correct ones.

In connectionist modelling, there is a way of learning called "error-driven learning." Here's how it works: Imagine you are playing a game and you need to give answers to questions. Let's say the correct answers to the questions are already known. During the game, the responses you give are compared to the correct answers. If your response is incorrect, you will learn from this mistake and try to improve next time. This learning mechanism helps you to understand and remember the right answers better. So basically, error-driven learning is a method where your actual responses are compared to the correct ones, helping you learn from your mistakes.

 

## What are the example case of back-propagation?

1\. The original paper on backpropagation: The foundational work on backpropagation was introduced by Paul Werbos in 1974, which presented the concept of backpropagation as a method for training neural networks. This paper laid the groundwork for subsequent research in the field.

2\. Gradient-based learning applied to document recognition: Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner published a seminal paper in 1998 titled "Gradient-based learning applied to document recognition," which demonstrated the practical application of backpropagation for training convolutional neural networks (CNNs) in handwritten digit recognition. This research significantly improved the accuracy and efficiency of computer vision tasks.

3\. Deep learning: Geoffrey Hinton, Yoshua Bengio, and others have made significant contributions to deep learning using backpropagation. Their research focuses on developing deep neural networks capable of learning high-level representations of data. Notable papers include "Deep Neural Networks for Acoustic Modeling in Speech Recognition" by Hinton et al. in 2012 and "Deep Learning" by Bengio et al. in 2015.

4\. Adaptive learning algorithms for backpropagation: The work by Rumelhart, Hinton, and Williams in 1988 introduced the use of adaptive learning algorithms, such as the backpropagation algorithm with momentum, to improve the convergence of neural network training. This research was influential in enhancing the efficiency and success of backpropagation for training deep neural networks.

5\. Backpropagation learning through time: In the field of recurrent neural networks (RNNs), the concept of backpropagation through time (BPTT) was introduced by Paul Werbos in 1988. This research extended the backpropagation algorithm to train RNNs, enabling the networks to effectively capture temporal dependencies in sequential data.

These are just a few notable research papers on backpropagation. Many other studies have expanded upon and refined the algorithm, leading to breakthroughs in various fields such as computer vision, natural language processing, and reinforcement learning.

 

## What are other terms related to back-propagation?

1\. Backward propagation: This term is often used interchangeably with back-propagation and refers to the process of propagating errors backward through the neural network to adjust the weights and biases of the network.

2\. Gradient descent: This is an optimization algorithm used in back-propagation to minimize the error or cost function by iteratively adjusting the network's weights and biases in the direction of steepest descent of the cost function.

3\. Error back-propagation: This refers to the process of calculating and propagating the error or loss backward through the network. It involves computing the derivatives of the cost function with respect to the weights and biases of the network, and using these derivatives to update the network parameters accordingly.

4\. Delta rule: This is a mathematical formula used in back-propagation to calculate the error signal that is propagated backward through the network. It is based on the chain rule of calculus and represents the change in the error with respect to changes in the network parameters.

5\. Back-propagation algorithm: This is the specific algorithm used to train a neural network using back-propagation. It involves forward propagation of inputs to calculate the predicted outputs, and then backward propagation of the error to adjust the network parameters.

6\. Multilayer perceptron: This term refers to a specific type of neural network architecture consisting of multiple layers of interconnected nodes or neurons. Back-propagation is commonly used to train such networks.

7\. Vanishing gradients: This refers to the problem that can occur in deep neural networks during back-propagation when the gradients of the cost function become extremely small as they are propagated backward through many layers. This can slow down or impede the learning process.

8\. Back-propagation through time (BPTT): This is a variant of back-propagation commonly used in recurrent neural networks (RNNs). It involves unrolling the recurrent network through time and applying back-propagation to compute the gradients and update the parameters.

9\. Stochastic gradient descent (SGD): This is a variation of gradient descent used in back-propagation, where instead of calculating the gradient of the entire training dataset, the gradient is estimated using a randomly selected subset or batch of training samples. It can speed up the training process, especially for large datasets.

10\. Mini-batch gradient descent: This is another variation of gradient descent used in back-propagation, where the training dataset is divided into small batches, and the gradient is calculated and applied to update the network parameters after processing each batch. It strikes a balance between the computational efficiency of SGD and the stability of batch gradient descent.

 

## References for back-propagation

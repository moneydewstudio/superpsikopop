---
Title: "Definition of Artificial Neural Network Ann in Psychology"
Description: "Get to know the definition of artificial neural network ann according to psychologists."
Date: 2023-03-16T06:00:00+00:201
Image: "/images/posts/definition-of-artificial-neural-network-ann-in-psychology.jpg"
Authors: ["Robi Maulana"]
Categories: ["Glossary"]
Tags: 
Draft: FALSE
---




> See neural network.

## What is the definition of artificial neural network (ANN) according to Psychologists?

artificial neural network (ANN) is See neural network.

A neural network is like a special computer program that tries to learn things in a similar way to how our brain learns. Just like our brain has lots of tiny cells called neurons that work together to help us think and remember, a neural network is made up of many small computer parts called nodes.

These nodes are connected to each other and they pass information back and forth, just like how our neurons send signals to each other. When a neural network is given some data, it tries to find patterns and make predictions based on what it has learned from this data.

For example, let's say you want to teach a neural network how to recognize pictures of cats. You would show it thousands of pictures of cats and tell it "These are cats!" Then the neural network would try to figure out what features or characteristics make up a cat, like pointy ears or a furry body. Over time, as it sees more and more pictures of cats, it becomes better at recognizing them.

Just like when we learn something new, the more information a neural network is exposed to, the better it gets at understanding and recognizing things. It's like a big puzzle-solving program that uses patterns and connections to understand and make decisions.

 

## What are the example case of artificial neural network (ANN)?

1\. The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain: This research, conducted by Frank Rosenblatt in 1958, introduced the concept of the perceptron, a basic artificial neural network model.

2\. Backpropagation: Learning Representations by Error Propagation: Proposed by Paul Werbos in 1974, this research presented the backpropagation algorithm, which allows artificial neural networks to learn and adjust weights in order to make accurate predictions.

3\. LSTM: Long Short-Term Memory: In 1997, Sepp Hochreiter and Jürgen Schmidhuber introduced LSTM, a type of recurrent neural network (RNN) with memory cells. This research significantly improved the network's ability to process and remember long-term dependencies.

4\. Convolutional Neural Networks for Image Classification: This seminal research, carried out by Yann LeCun, Yoshua Bengio, and others in the 1990s, demonstrated the effectiveness of convolutional neural networks (CNNs) in image recognition tasks. CNNs revolutionized the field of computer vision.

5\. Generative Adversarial Networks (GANs): Proposed by Ian Goodfellow and his colleagues in 2014, GANs use two neural networks, a generator and a discriminator, to generate highly realistic synthetic data. GANs have yielded significant advancements in areas such as image synthesis, text generation, and data augmentation.

6\. AlphaGo: Mastering the Game of Go with Deep Neural Networks and Tree Search: This landmark research by DeepMind in 2016 showcased how deep neural networks, combined with Monte Carlo tree search techniques, enabled an AI system to defeat world-champion Go players. This achievement demonstrated the power of deep learning for complex strategy games.

7\. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: In 2018, researchers at Google introduced BERT, a transformer-based neural network model that significantly advanced the state-of-the-art in natural language processing (NLP) tasks such as text classification, question-answering, and sentiment analysis.

8\. Transformer: Attention Is All You Need: This research by Vaswani et al. in 2017 introduced the transformer model, which relies solely on self-attention mechanisms without any recurrent or convolutional layers. The transformer architecture has become a standard model for various NLP tasks and has demonstrated superior performance compared to traditional models.

9\. Reinforcement Learning with Deep Q-Networks (DQN): In 2013, researchers at DeepMind introduced DQN, which combined deep neural networks with reinforcement learning techniques. DQN achieved remarkable results in playing Atari games, showing the potential of using neural networks for reinforcement learning tasks in complex environments.

10\. GPT-3: Language Models are Few-Shot Learners: This 2020 research by OpenAI presented GPT-3, an incredibly large language model with 175 billion parameters. GPT-3 demonstrated impressive language understanding and generation capabilities, further illustrating the power of large-scale neural networks in natural language processing.

 

## What are other terms related to artificial neural network (ANN)?

1\. Deep Learning: A subset of machine learning that utilizes artificial neural networks with multiple layers to extract features and make predictions.

2\. Neuron: The basic computational unit of an artificial neural network, which receives inputs, applies weights and biases, and produces an output.

3\. Activation Function: A non-linear mathematical function that determines the output of a neuron based on its input.

4\. Backpropagation: A learning algorithm used to train artificial neural networks by adjusting the weights and biases based on the calculated errors between predicted and actual outputs.

5\. Gradient Descent: An optimization algorithm used in training neural networks to minimize the error or loss function by iteratively adjusting the weights and biases.

6\. Convolutional Neural Network (CNN): A type of artificial neural network designed for image recognition and processing, using convolutional layers to extract features.

7\. Recurrent Neural Network (RNN): A type of artificial neural network that can process sequential data by introducing recurrent connections and feedback loops to retain memory of past inputs.

8\. Long Short-Term Memory (LSTM): A type of recurrent neural network architecture that resolves the vanishing gradient problem and enables the network to retain information over longer sequences.

9\. Self-Organizing Maps (SOM): A type of unsupervised artificial neural network that produces a low-dimensional representation of high-dimensional input data, facilitating clustering and visualization.

10\. Autoencoder: A type of artificial neural network that is trained to reconstruct its input data, enabling dimensionality reduction and feature extraction.

 

## References for artificial neural network (ANN)

1\. McCulloch, W. S., & Pitts, W. H. (1943). A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5(4), 115-133.

2\. Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6), 386-408.

3\. Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323(6088), 533-536.

4\. LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. nature, 521(7553), 436-444.

5\. Haykin, S. (1999). Neural networks: a comprehensive foundation (Vol. 2). New York: Macmillan College Publishing Company.

6\. Widrow, B., & Hoff, M. E. (1960). Adaptive switching circuits. IRE WESCON Convention Record, 4(3), 96-104.

7\. Hebb, D. O. (1949). The organization of behavior: A neuropsychological theory. Wiley.

8\. Kohonen, T. (1982). Self-organized formation of topologically correct feature maps. Biological cybernetics, 43(1), 59-69.

9\. Bishop, C. M. (1995). Neural networks for pattern recognition. New York: Oxford University Press.

10\. Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks, 61, 85-117.
